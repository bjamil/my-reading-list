My Reading List 
---

Publications, books, and web pages I've been reading or am planning on reading. 

Why
---
I've been trying to level up recently on ML, LLMs, NLU, etc. and whenever I read a paper, I feel there are ten others I should read as well :) . This repo is to better track what I've read and what I want to read and jot some learnings along the way. 

I also want to give this [Learning in Public](https://www.swyx.io/learn-in-public) thing a shot. Let's see how it goes! 


ML Reading List
---


## General 

| Paper | Read Date | Last Revise Date|Notes|
| --- | --- |---|---|
|[Evaluating Large Language Models Trained on Code](https://arxiv.org/abs/2107.03374)|2023-03-12|||
|[Understanding HTML with Large Language Models](https://arxiv.org/abs/2210.03945)|2023-03-12|2022-10-08|[Notes](reading_notes/understanding_html_with_large_language_models.md)|
|[Multi-Task Sequence to Sequence Learning](https://arxiv.org/abs/1511.06114)||2016-03-01||
|**[Emergent Abilities of Large Language Models](https://arxiv.org/abs/2206.07682)**||2022-10-06||
|[BLOOM: A 176B-Parameter Open-Access Multilingual Language Model](https://arxiv.org/abs/2211.05100)||2022-12-11||
|**[Finetuned Language Models Are Zero-Shot Learners](https://arxiv.org/abs/2109.01652)**||2022-02-08||
|[LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)||2023-03-27||
|**[Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)**||2022-03-04||
|**[HTLM: Hyper-Text Pre-Training and Prompting of Language Models](https://arxiv.org/abs/2107.06955)**||2021-07-14||
|**[Environment Generation for Zero-Shot Compositional Reinforcement Learning](https://arxiv.org/abs/2201.08896)**||2022-01-21||
|[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)||||
|[LaMDA: Language Models for Dialog Applications](https://arxiv.org/abs/2201.08239)||||


## Training Speedups/Scaling

| Paper | Read Date | Last Revise Date|Notes|
| --- | --- |---|---|
|[MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://arxiv.org/abs/2004.02984)||||
|[PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/abs/2204.02311)||||
|[ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)||||
|[Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556)||2022-03-29||
|[GLaM: Efficient Scaling of Language Models with Mixture-of-Experts](https://arxiv.org/abs/2112.06905)||2021-12-13||


## Non-LLMs
| Paper | Read Date | Last Revise Date|Notes|
| --- | --- |---|---|
|**[World of Bits: An Open-Domain Platform for Web-Based Agents](http://proceedings.mlr.press/v70/shi17a/shi17a.pdf)**||2017||
|[Reinforcement Learning on Web Interfaces Using Workflow-Guided Exploration](https://arxiv.org/abs/1802.08802)||||
|[User-Driven Automation of Web Form Filling](https://link.springer.com/chapter/10.1007/978-3-642-39200-9_16)||2013||
|[Learning Transferable Visual Models from Natural Language Supervision](https://arxiv.org/abs/2103.00020)||2021-02-26||
|[Learning to Generate Reviews and Discovering Sentiment](https://arxiv.org/abs/1704.01444)||2017-04-06||
|[WikiGraphs: A Wikipedia Text - Knowledge Graph Paired Dataset](https://arxiv.org/abs/2107.09556)||2021-07-20||
|[Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)||||
|[Extracting Structured Data from Templatic Documents](https://blog.research.google/2020/06/extracting-structured-data-from.html)||2020-06-12||

## Bloom Filters
| Read Date | Resource | Notes | 
| --- | --- | --- | 
| 2024-01-30 | [Bloom Filters by ByteByteGo](https://www.youtube.com/watch?v=V3pzxngeLqw) | Gives a decent intuition |
| 2024-01-30 | [What are Bloom Filters?](https://www.youtube.com/watch?v=kfFacplFY4Y) | Not the best example. prev vid was better |
| 2024-01-30 | [Advancing Spark - Bloom Filter Indexes in Databricks Delta](https://www.youtube.com/watch?v=gaPQt0oPKLI) | Interesting, but more about delta than spark, as the title implies | 
| | [The Case for Learned Index Structures](https://arxiv.org/abs/1712.01208)||
|| [Optimizing Learned Bloom Filters by Sandwiching](https://arxiv.org/abs/1803.01474)||


## Quantization, Model Compression & Optimization
| Read Date | Resource | Notes | 
|---|---|---|
| | [Permute, Quantize, and Fine-tune: Efficient Compression of Neural Networks](https://arxiv.org/abs/2010.15703) ||


## Blog Posts I Liked
| Read Date | Post | Notes |
| ---| --- | --- |
| 2024-01-30 |[How we reduced our text similarity runtime by 99.96% (Microsoft)](https://medium.com/data-science-at-microsoft/how-we-reduced-our-text-similarity-runtime-by-99-96-e8e4b4426b35) | I skimmed through it. Seems interesting and worth a reread |
| 2024-01-30 |[How Roblox Reduces Spark Join Query Costs With Machine Learning Optimized Bloom Filters](https://blog.roblox.com/2023/11/roblox-reduces-spark-join-query-costs-machine-learning-optimized-bloom-filters/)| I wonder if this can be applied to other use cases too and not just fact/dim tables. Interesting read. | 


## Blog Posts to Read
| Post | Notes | 
| --- | --- | 
| [Using machine learning to index text from billions of images](https://dropbox.tech/machine-learning/using-machine-learning-to-index-text-from-billions-of-images) (Dropbox) | Curious abouth the OCR/PDF text extraction part here. Need some caffiene in me to read this. | 



